import requests
import re
import json
import urllib3
from bs4 import BeautifulSoup

# SSL uyarÄ±larÄ±nÄ± kapat
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AdvancedMonoScraper:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        }

    def get_active_domain(self):
        """Domainleri tarar ve aktif olanÄ± dÃ¶ndÃ¼rÃ¼r."""
        print("ğŸ” Aktif domain taranÄ±yor...")
        # Senin brute-force mantÄ±ÄŸÄ±n en gÃ¼venlisi
        for sayi in range(530, 560):
            domain = f"https://monotv{sayi}.com"
            try:
                # Sadece ana sayfayÄ± hÄ±zlÄ±ca kontrol et
                r = self.session.get(domain, timeout=4, verify=False, headers=self.headers)
                if r.status_code == 200:
                    print(f"âœ… Aktif site bulundu: {domain}")
                    return domain.rstrip('/')
            except:
                continue
        return None

    def find_m3u8_server(self, html):
        """YayÄ±n sunucusunu HTML iÃ§inden otomatik ayÄ±klar."""
        # Senin verdiÄŸin sunucuyu da kapsayan geniÅŸ regex
        pattern = r'["\'](https?://[a-z0-9.-]+\.(?:sbs|xyz|live|pw|site|cfd|tv)/)'
        match = re.search(pattern, html)
        if match:
            server = match.group(1)
            print(f"ğŸŒ YayÄ±n sunucusu: {server}")
            return server
        return "https://rei.zirvedesin201.cfd/" # Fallback

    def scrape(self):
        domain = self.get_active_domain()
        if not domain: return

        try:
            r = self.session.get(domain, headers=self.headers, verify=False)
            r.encoding = 'utf-8'
            soup = BeautifulSoup(r.text, 'html.parser')
            
            m3u8_server = self.find_m3u8_server(r.text)
            channels = []

            # 1. MaÃ§lar ve Kanallar iÃ§in Ortak Tarama
            # Sitedeki tÃ¼m 'channel?id=' iÃ§eren linkleri bul
            links = soup.find_all('a', href=re.compile(r'channel\?id='))
            
            for link in links:
                cid_match = re.search(r'id=([^&"\']+)', link['href'])
                if not cid_match: continue
                cid = cid_match.group(1)

                # Ä°sim AyÄ±klama (GeliÅŸmiÅŸ)
                # Ã–nce iÃ§indeki span veya div'lere bak, yoksa dÃ¼z metni al
                home = link.find(class_="home")
                away = link.find(class_="away")
                
                if home and away:
                    name = f"{home.get_text(strip=True)} - {away.get_text(strip=True)}"
                    group = "CANLI MACLAR"
                else:
                    name = link.get_text(strip=True).replace("7/24", "").strip()
                    group = "7/24 KANALLAR"

                if not name: name = cid.upper()

                channels.append({
                    "name": name,
                    "group": group,
                    "url": f"{m3u8_server}{cid}/mono.m3u8"
                })

            # 2. M3U OluÅŸturma
            if channels:
                with open("mono_list.m3u", "w", encoding="utf-8") as f:
                    f.write("#EXTM3U\n")
                    for ch in channels:
                        f.write(f'#EXTINF:-1 group-title="{ch["group"]}",{ch["name"]}\n')
                        f.write(f'#EXTVLCOPT:http-referrer={domain}/\n')
                        f.write(f'{ch["url"]}\n')
                print(f"ğŸ BaÅŸarÄ±lÄ±! {len(channels)} kanal mono_list.m3u dosyasÄ±na yazÄ±ldÄ±.")
            else:
                print("âš ï¸ HiÃ§ kanal bulunamadÄ±.")

        except Exception as e:
            print(f"âŒ Hata: {e}")

if __name__ == "__main__":
    AdvancedMonoScraper().scrape()
