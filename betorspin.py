import requests
import re
import sys
import urllib3
from bs4 import BeautifulSoup

# SSL uyarÄ±larÄ±nÄ± kapat
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Siteye Ã¶zel ayarlar
TARGET_SITE = "https://63betorspintv.live"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": TARGET_SITE + "/"
}

# YasaklÄ± (sahte) reklam domainleri
BANNED_DOMAINS = ["gstatic", "jsdelivr", "google", "doubleclick", "analytics", "facebook", "twitter"]

def resolve_base_url(channel_id):
    """Kanal sayfasÄ±ndan gerÃ§ek yayÄ±n sunucusunu (base_url) Ã§Ã¶zer."""
    target = f"{TARGET_SITE}/channel?id={channel_id}"
    try:
        r = requests.get(target, headers=HEADERS, timeout=10, verify=False)
        # m3u8 linklerini bul
        urls = re.findall(r'["\'](https?://[^\s"\']+?/)[\w\-]+\.m3u8', r.text)
        for link in urls:
            if not any(banned in link for banned in BANNED_DOMAINS):
                return link
        # Alternatif geniÅŸ arama
        alt_urls = re.findall(r'["\'](https?://[a-z0-9.-]+\.(?:sbs|xyz|live|me|net|com|pw|site|club)/)', r.text)
        for link in alt_urls:
            if not any(banned in link for banned in BANNED_DOMAINS):
                return link
    except: pass
    return None

def main():
    try:
        print(f"ğŸ“¡ {TARGET_SITE} taranÄ±yor...")
        
        # 1. YayÄ±n sunucusunu bul (beIn Sports 1 Ã¼zerinden)
        base_url = resolve_base_url("yayinzirve")
        if not base_url:
            sys.exit("âŒ YayÄ±n sunucusu bulunamadÄ±.")
        print(f"âœ… YayÄ±n sunucusu yakalandÄ±: {base_url}")

        # 2. Ana sayfayÄ± Ã§ek ve analiz et
        r = requests.get(TARGET_SITE, headers=HEADERS, timeout=10, verify=False)
        r.encoding = "utf-8"
        soup = BeautifulSoup(r.text, "html.parser")

        final_list = []

        # --- MAÃ‡LAR KISMI (live-content) ---
        matches_area = soup.find(id="matches-content")
        if matches_area:
            for a in matches_area.find_all("a", href=re.compile(r'id=')):
                cid = re.search(r'id=([^&]+)', a["href"]).group(1)
                event = a.find(class_="event").get_text(strip=True) if a.find(class_="event") else "MaÃ§"
                home = a.find(class_="home").get_text(strip=True) if a.find(class_="home") else ""
                away = a.find(class_="away").get_text(strip=True) if a.find(class_="away") else ""
                title = f"{event} | {home} - {away}"
                final_list.append({"cid": cid, "title": title, "group": "CanlÄ± MaÃ§lar"})

        # --- KANALLAR KISMI (channels-content) ---
        channels_area = soup.find(id="channels-content")
        if channels_area:
            for a in channels_area.find_all("a", href=re.compile(r'id=')):
                cid = re.search(r'id=([^&]+)', a["href"]).group(1)
                name = a.find(class_="home").get_text(strip=True) if a.find(class_="home") else cid
                final_list.append({"cid": cid, "title": name, "group": "TV KanallarÄ±"})

        # 3. M3U DosyasÄ±nÄ± YazdÄ±r
        with open("betorspin.m3u", "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            for item in final_list:
                f.write(f'#EXTINF:-1 group-title="{item["group"]}",{item["title"]}\n')
                f.write(f'#EXTVLCOPT:http-user-agent={HEADERS["User-Agent"]}\n')
                f.write(f'#EXTVLCOPT:http-referrer={TARGET_SITE}/\n')
                f.write(f'{base_url}{item["cid"]}.m3u8\n')

        print(f"ğŸ TAMAM â†’ {len(final_list)} kanal 'betorspin.m3u' dosyasÄ±na ham linklerle eklendi.")

    except Exception as e:
        print(f"âŒ Hata: {e}")

if __name__ == "__main__":
    main()
