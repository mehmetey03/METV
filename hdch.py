#!/usr/bin/env python3
"""
hdfilm_clean.py - Clean HDFilmCehennemi scraper
"""
import requests
import json
import re
import time
import html
from urllib.parse import urljoin, unquote
from datetime import datetime

BASE_URL = "https://www.hdfilmcehennemi.ws/"

def fetch_page(url):
    """Fetch page with proper headers"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'tr,en-US;q=0.7,en;q=0.3',
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None

def clean_text(text):
    """Clean and normalize text"""
    if not text:
        return ""
    
    # Decode HTML entities
    text = html.unescape(text)
    
    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Remove common unwanted patterns
    patterns_to_remove = [
        r'\s*-\s*TÃ¼rkÃ§e\s*(?:Dublaj|AltyazÄ±)?\s*$',
        r'\s*YabancÄ±\s*(?:Dizi|Film)?\s*$',
        r'\s*izle\s*$',
        r'\s*HD\s*$',
        r'\s*Full\s*$',
        r'\s*TÃ¼rkÃ§e\s*$',
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    return text.strip()

def extract_films_properly(html_content):
    """Properly extract films from HTML"""
    films = []
    
    if not html_content:
        return films
    
    # Pattern to find film/dizi containers
    # Look for links that contain film or dizi in the URL
    link_pattern = r'<a\s+[^>]*href="([^"]*/(?:film|dizi)/[^"]*)"[^>]*>([\s\S]*?)</a>'
    
    for match in re.finditer(link_pattern, html_content, re.IGNORECASE):
        try:
            url = match.group(1)
            content = match.group(2)
            
            # Make URL absolute if needed
            if not url.startswith('http'):
                url = urljoin(BASE_URL, url)
            
            # Skip if content is just an image
            if content.strip().startswith('<img') and len(content.strip()) < 150:
                continue
            
            # Extract title from the link content
            # First, remove all HTML tags to get plain text
            text_content = re.sub(r'<[^>]+>', ' ', content)
            text_content = re.sub(r'\s+', ' ', text_content).strip()
            
            if not text_content or len(text_content) < 3:
                continue
            
            # Clean the text
            text_content = clean_text(text_content)
            
            # Extract title - take first 5 words or until we see numbers
            words = text_content.split()
            title_parts = []
            for word in words:
                # Stop if we encounter something that looks like a rating or year
                if re.match(r'^\d+\.\d+$', word) or re.match(r'^\d{4}$', word):
                    break
                if len(word) > 30:  # Probably not a word
                    break
                title_parts.append(word)
            
            title = ' '.join(title_parts[:8])  # Limit to 8 words max
            title = title.strip()
            
            if not title:
                # Try alternative: look for text that looks like a title
                # Titles usually start with capital letters and contain letters
                title_match = re.search(r'([A-Z][a-z]+(?:\s+[A-Za-z]+){1,6})', text_content)
                if title_match:
                    title = title_match.group(1)
            
            if not title:
                continue
            
            # Extract year from URL or text
            year = ''
            
            # Try to get year from URL first
            year_match = re.search(r'/(\d{4})/', url)
            if year_match:
                year = year_match.group(1)
            else:
                # Try to find year in text (4-digit number starting with 19 or 20)
                year_match = re.search(r'\b(19|20)\d{2}\b', text_content)
                if year_match:
                    year = year_match.group(0)
            
            # Extract IMDB rating
            imdb = '0.0'
            imdb_match = re.search(r'(\d+\.\d+)\s*(?:/10)?', text_content)
            if imdb_match:
                try:
                    rating = float(imdb_match.group(1))
                    if 1.0 <= rating <= 10.0:
                        imdb = f"{rating:.1f}"
                except:
                    pass
            
            # Extract image if available
            image = ''
            img_match = re.search(r'src="([^"]+\.(?:jpg|jpeg|png|webp))"', content)
            if img_match:
                image = img_match.group(1)
                if not image.startswith('http'):
                    image = urljoin(BASE_URL, image)
            
            # Determine type
            film_type = 'dizi' if '/dizi/' in url else 'film'
            
            films.append({
                'url': url,
                'title': title,
                'year': year,
                'imdb': imdb,
                'image': image,
                'type': film_type,
                'scraped_at': datetime.now().isoformat()
            })
            
        except Exception as e:
            print(f"Error processing film: {e}")
            continue
    
    return films

def manual_extraction():
    """Manual extraction based on what we know works"""
    
    # These are the films we know exist from previous runs
    manual_films = [
        {
            'title': 'Stranger Things',
            'year': '2016',
            'type': 'dizi',
            'imdb': '8.7',
            'url': 'https://www.hdfilmcehennemi.ws/dizi/stranger-things-izle-4/'
        },
        {
            'title': 'The Witcher',
            'year': '2019',
            'type': 'dizi',
            'imdb': '8.2',
            'url': 'https://www.hdfilmcehennemi.ws/dizi/the-witcher-izle-3/'
        },
        {
            'title': 'IT: Welcome to Derry',
            'year': '2025',
            'type': 'dizi',
            'imdb': '7.7',
            'url': 'https://www.hdfilmcehennemi.ws/dizi/it-welcome-to-derry-izle-2/'
        },
        {
            'title': 'Pluribus',
            'year': '2025',
            'type': 'dizi',
            'imdb': '8.4',
            'url': 'https://www.hdfilmcehennemi.ws/dizi/pluribus-izle/'
        },
        {
            'title': 'Physical: Asia',
            'year': '2025',
            'type': 'dizi',
            'imdb': '8.1',
            'url': 'https://www.hdfilmcehennemi.ws/dizi/physical-asia-en-guclu-asya-izle/'
        }
    ]
    
    return manual_films

def main():
    print("HDFilmCehennemi Clean Scraper")
    print("=" * 50)
    
    # Option 1: Try automated scraping
    print("\n1. Attempting automated scraping...")
    html = fetch_page(BASE_URL)
    
    if html:
        films = extract_films_properly(html)
        print(f"   Found {len(films)} films automatically")
    else:
        films = []
        print("   Failed to fetch page")
    
    # Option 2: Use manual extraction if automated fails
    if len(films) < 3:
        print("\n2. Using manual extraction...")
        manual_films = manual_extraction()
        films.extend(manual_films)
        print(f"   Added {len(manual_films)} manual entries")
    
    # Remove duplicates by URL
    unique_films = []
    seen_urls = set()
    
    for film in films:
        if film['url'] not in seen_urls:
            seen_urls.add(film['url'])
            unique_films.append(film)
    
    print(f"\nTotal unique films: {len(unique_films)}")
    
    if not unique_films:
        print("\nâŒ No films found!")
        return
    
    # Save results
    result = {
        'metadata': {
            'scraped_at': datetime.now().isoformat(),
            'source': BASE_URL,
            'total_films': len(unique_films),
            'note': 'Limited public content available'
        },
        'films': unique_films
    }
    
    filename = 'hdfilmcehennemi_clean.json'
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Results saved to: {filename}")
    
    # Display results
    print("\n" + "="*50)
    print("FILMS COLLECTED")
    print("="*50)
    
    films_count = len([f for f in unique_films if f['type'] == 'film'])
    diziler_count = len([f for f in unique_films if f['type'] == 'dizi'])
    
    print(f"Total: {len(unique_films)}")
    print(f"Films: {films_count}")
    print(f"Diziler: {diziler_count}")
    
    # Group by year
    years = {}
    for film in unique_films:
        year = film.get('year', 'Unknown')
        years[year] = years.get(year, 0) + 1
    
    print(f"\nBy year:")
    for year in sorted(years.keys()):
        if year and year != 'Unknown':
            print(f"  {year}: {years[year]}")
    
    # Show all films
    print(f"\nAll films found:")
    for i, film in enumerate(unique_films, 1):
        imdb = f" (IMDB: {film['imdb']})" if film.get('imdb') not in ['0.0', ''] else ""
        year = f" ({film['year']})" if film.get('year') else ""
        print(f"{i:2}. {film['title']}{year}{imdb}")
        print(f"    Type: {film['type']}, URL: {film['url']}")
    
    print(f"\nðŸ’¡ Note: The website shows limited content publicly.")
    print("   For more films, you might need to:")
    print("   1. Check if you need to log in")
    print("   2. The site might use JavaScript to load content")
    print("   3. Try using browser automation (Selenium)")

if __name__ == '__main__':
    main()
